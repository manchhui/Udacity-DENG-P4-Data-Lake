{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Lake - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, col, lit, concat\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "input_data = config['AWS']['INPUT_DATA']\n",
    "output_data = config['LOCAL']['OUTPUT_DATA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()\n",
    "spark.sql(\"set spark.sql.parquet.compression.codec=gzip\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long\n",
    "logdataSchema = R([\n",
    "    Fld(\"artist\",Str()),\n",
    "    Fld(\"auth\",Str()),\n",
    "    Fld(\"firstName\",Str()),\n",
    "    Fld(\"gender\",Str()),\n",
    "    Fld(\"itemInSession\",Long()),\n",
    "    Fld(\"lastName\",Str()),\n",
    "    Fld(\"length\",Dbl()),\n",
    "    Fld(\"level\",Str()),\n",
    "    Fld(\"location\",Str()),\n",
    "    Fld(\"method\",Str()),\n",
    "    Fld(\"page\",Str()),\n",
    "    Fld(\"registration\",Dbl()),\n",
    "    Fld(\"sessionId\",Long()),\n",
    "    Fld(\"song\",Str()),\n",
    "    Fld(\"status\",Long()),\n",
    "    Fld(\"ts\",Long()),\n",
    "    Fld(\"userAgent\",Str()),\n",
    "    Fld(\"userId\",Str()),\n",
    "])\n",
    "\n",
    "songdataSchema = R([\n",
    "    Fld(\"artist_id\",Str()),\n",
    "    Fld(\"artist_latitude\",Dbl()),\n",
    "    Fld(\"artist_location\",Str()),\n",
    "    Fld(\"artist_longitude\",Dbl()),\n",
    "    Fld(\"artist_name\",Str()),\n",
    "    Fld(\"duration\",Dbl()),\n",
    "    Fld(\"num_songs\",Long()),\n",
    "    Fld(\"song_id\",Str()),\n",
    "    Fld(\"title\",Str()),\n",
    "    Fld(\"year\",Long()),\n",
    "])\n",
    "\n",
    "log_data = spark.createDataFrame(sc.emptyRDD(), logdataSchema)\n",
    "song_data = spark.createDataFrame(sc.emptyRDD(), songdataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getFilepaths(prefix):\n",
    "    \n",
    "    s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "                   )\n",
    "    \n",
    "    filepaths=[]\n",
    "    if 's3a://' in input_data:\n",
    "        my_bucket = s3.Bucket('udacity-dend')\n",
    "        objects = my_bucket.objects.filter(Prefix=prefix)\n",
    "        for obj in objects:\n",
    "            path, filename = os.path.split(obj.key)\n",
    "            if '.json' in filename:\n",
    "                filespaths = filepaths.append(input_data+'/'+obj.key)\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(input_data+'/'+prefix):\n",
    "            files = glob.glob(os.path.join(root,'*.json'))\n",
    "            for f in files:\n",
    "                filespaths = filepaths.append(os.path.abspath(f))\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .JSON filepaths - In Progress...\n",
      "Extracting .JSON filepaths - Completed, Time Taken: 0:00:13.390887\n",
      "Extracting .JSON log_data - In Progress...\n",
      "Extracting .JSON log_data - Completed, Time Taken: 0:00:05.224602\n",
      "Extracting .JSON song_data - In Progress...\n",
      "Extracting .JSON song_data - Completed, Time Taken: 0:00:10.466239\n"
     ]
    }
   ],
   "source": [
    "# get all filepaths - not used in final script.\n",
    "start = datetime.now()\n",
    "print('Extracting .JSON filepaths - In Progress...')\n",
    "logFilepaths = getFilepaths('log_data')\n",
    "songFilepaths = getFilepaths('song_data')\n",
    "stop = datetime.now()\n",
    "total = stop - start\n",
    "print('Extracting .JSON filepaths - Completed, Time Taken: {}'.format(total))\n",
    "\n",
    "# get total number of filepaths to be loaded - not used in final script.\n",
    "start = datetime.now()\n",
    "print('Extracting .JSON log_data - In Progress...')\n",
    "log_data = spark.read.json(logFilepaths, schema=logdataSchema)\n",
    "stop = datetime.now()\n",
    "total = stop - start\n",
    "print('Extracting .JSON log_data - Completed, Time Taken: {}'.format(total))\n",
    "\n",
    "# loading song logs, only 50 / 14896 filepaths used - not used in final script.\n",
    "start = datetime.now()\n",
    "print('Extracting .JSON song_data - In Progress...')\n",
    "song_data = spark.read.json(songFilepaths[:50], schema=songdataSchema)\n",
    "stop = datetime.now()\n",
    "total = stop - start\n",
    "print('Extracting .JSON song_data - Completed, Time Taken: {}'.format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data.limit(1).toPandas().T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(song_data.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data.limit(1).toPandas().T.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Process Song Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming .JSON log_data files - In Progress, Count of: \"8056\" files...\n",
      "Transforming .JSON log_data - Completed, Time Taken: 0:00:05.256818\n",
      "Loading/Writing .JSON log_data files - In Progress, Count of: \"8056\" files...\n",
      "Loading/Writing .JSON log_data - Completed, Time Taken: 0:00:29.289841\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print('Transforming .JSON log_data files - In Progress, Count of: \"{}\" files...'.format(log_data.count()))\n",
    "\n",
    "# get filepath to song data file\n",
    "df = song_data\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df.select('song_id','title','artist_id','year','duration')\n",
    "songs_table = songs_table.dropDuplicates()\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df.select('artist_id','artist_name','artist_location','artist_latitude','artist_longitude')\n",
    "artists_table = artists_table.dropDuplicates()\n",
    "\n",
    "stop = datetime.now()\n",
    "total = stop - start\n",
    "print('Transforming .JSON log_data - Completed, Time Taken: {}'.format(total))\n",
    "\n",
    "start = datetime.now()\n",
    "print('Loading/Writing .JSON log_data files - In Progress, Count of: \"{}\" files...'.format(log_data.count()))\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_tablewrite=songs_table.repartition('year','artist_id')\n",
    "songs_tablewrite.write.mode(\"overwrite\").partitionBy('year','artist_id').parquet(output_data+'/'+'songs_table.parquet')\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.mode(\"overwrite\").parquet(output_data+'/'+'artists_table')\n",
    "\n",
    "stop = datetime.now()\n",
    "total = stop - start\n",
    "print('Loading/Writing .JSON log_data - Completed, Time Taken: {}'.format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Process Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print('Transforming .JSON song_data - In Progress, Count of: \"{}\" files...'.format(song_data.count()))\n",
    "\n",
    "\n",
    "# get filepath to log data file\n",
    "\n",
    "\n",
    "# read log data file\n",
    "df = log_data\n",
    "\n",
    "# filter by actions for song plays\n",
    "df=df.where(col(\"page\") == \"NextSong\")\n",
    "\n",
    "# extract columns for users table   \n",
    "users_table = df.withColumn(\"row_number\",F.row_number()\\\n",
    "                            .over(Window.partitionBy(df.userId, df.firstName, df.lastName, df.gender)\\\n",
    "                            .orderBy(df.ts.desc())))\\\n",
    "                            .filter(F.col(\"row_number\")==1)\\\n",
    "                            .drop(\"row_number\")\\\n",
    "                            .select('userId','firstName','lastName','gender','level')\n",
    "\n",
    "\n",
    "\n",
    "# create timestamp / datetime column from original timestamp column\n",
    "df = df.withColumn(\"timestamp\", F.to_timestamp(df['ts']/1000))\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "col_list = ['song','artist','length']\n",
    "\n",
    "song_df = df.select(col(\"timestamp\").alias(\"start_time\"), \n",
    "                    col(\"userid\").alias(\"user_id\"),\n",
    "                    col(\"level\"),\n",
    "                    col(\"sessionid\").alias(\"session_id\"),\n",
    "                    col(\"location\"),\n",
    "                    col(\"useragent\").alias(\"user_agent\"),\n",
    "                    col(\"song\"),\n",
    "                    col(\"artist\"),\n",
    "                    col(\"length\"),\n",
    "                    col(\"ts\"))\n",
    "song_df = song_df.withColumn('concat_id',concat(*col_list))\n",
    "\n",
    "col_list = ['title','artist_name','duration']\n",
    "song_df2 = songs_table.withColumn(\"artist_id\", col(\"artist_id\"))\\\n",
    "    .join(artists_table.withColumn(\"artist_id\", col(\"artist_id\")), on=\"artist_id\")\\\n",
    "    .select(\"song_id\",\"title\",\"artist_id\",\"artist_name\",\"duration\")\n",
    "song_df2 = song_df2.withColumn('concat_id',concat(*col_list))\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = song_df.join(song_df2, song_df[\"concat_id\"] == song_df2[\"concat_id\"], \"left\")\\\n",
    "    .select(\"start_time\",\"user_id\",\"level\",\"song_id\",\"artist_id\",\"session_id\",\"location\",\"user_agent\")\\\n",
    "    .dropDuplicates()\n",
    "songplays_table = songplays_table.withColumn(\"month\", F.month(\"start_time\"))\n",
    "songplays_table = songplays_table.withColumn(\"year\", F.year(\"start_time\"))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = songplays_table.select(col(\"start_time\").alias(\"time_stamp\"))\n",
    "time_table = time_table.withColumn(\"hour\", F.hour(\"time_stamp\"))\n",
    "time_table = time_table.withColumn(\"day\", F.dayofyear(\"time_stamp\"))\n",
    "time_table = time_table.withColumn(\"weekofyear\", F.weekofyear(\"time_stamp\"))\n",
    "time_table = time_table.withColumn(\"month\", F.month(\"time_stamp\"))\n",
    "time_table = time_table.withColumn(\"year\", F.year(\"time_stamp\"))\n",
    "time_table = time_table.withColumn(\"weekday\", F.dayofweek(\"time_stamp\"))\n",
    "\n",
    "stop = datetime.now()\n",
    "total = stop - start\n",
    "print('Transforming .JSON song_data - Completed, Time Taken: {}'.format(total))\n",
    "\n",
    "\n",
    "# start of loading/writing users / songplays and time tables\n",
    "start = datetime.now()\n",
    "print('Loading/Writing .JSON song_data - In Progress, Count of: \"{}\" files...'.format(song_data.count()))\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.mode(\"overwrite\").parquet(output_data+'/'+'users_table')\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month?\n",
    "songplays_table.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(output_data+'/'+'songplays_table')\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(output_data+'/'+'time_table')\n",
    "\n",
    "stop = datetime.now()\n",
    "total = stop - start\n",
    "print('Loading/Writing .JSON song_data - Completed, Time Taken: {}'.format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
